Our VTFF is built upon on two pre-trained ResNet18 [32] networks, and consists of two crucial components: i) attentional selective fusion, ii) multi-Layer Transformer encoder.For a given face image ğ¼ğ‘…ğºğµ with the size of ğ» Ã— ğ‘Š Ã— 3, we first get its LBP feature image with the size of ğ» Ã— ğ‘Š Ã— 1 and concatenate it to a feature image ğ¼ğ¿ğµğ‘ƒ with the size of ğ» Ã—ğ‘Š Ã—3. The feature extraction backbones are composed of two ResNet18 networks: one is for the RGB image and the other is for its LBP feature image. Particularly, we employ the first five stages of ResNet18 as the backbone to extract feature maps ğ‘‹ğ¿ğµğ‘ƒ and ğ‘‹ğ‘…ğºğµ with the size of ğ»/ğ‘… Ã— ğ‘Š/ğ‘… Ã— ğ¶ğ‘“, where R is the downsampling rate of ResNet18, ğ¶ğ‘“ is the channel number of the output of the stage 5. For simplicity, we denote ğ»ğ‘‘ =ğ»/ğ‘… and ğ‘Šğ‘‘ = ğ‘Š/ğ‘…. In this paper, ğ‘… = 32 and ğ» = ğ‘Š = 224.